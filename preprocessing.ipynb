{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add operation and database identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "rand_insert_ratio = 0.5\n",
    "\n",
    "def read_txt_file(fpath):\n",
    "    with open(fpath, 'r') as txt_file:\n",
    "        return txt_file.readlines()\n",
    "    \n",
    "def read_raw_data(fpath):\n",
    "    with open(fpath, 'r') as tbl_file:\n",
    "        tbl_reader = csv.reader(tbl_file, delimiter='|')\n",
    "        return list(tbl_reader)\n",
    "\n",
    "# for sf in ['0.2', '0.4', '0.6', '0.8', '1.0']:\n",
    "for sf in ['0.1']:\n",
    "\n",
    "    folder = \"tpch_datasets/data_sf\" + sf\n",
    "    files = ['nation.tbl', 'customer.tbl', 'orders.tbl', 'lineitem.tbl']\n",
    "\n",
    "    init_ops = []\n",
    "    update_ops = []\n",
    "\n",
    "    for file in files:\n",
    "        data = read_raw_data(folder + \"/\" + file)\n",
    "        \n",
    "        # insert: +, delete: -\n",
    "        insert_tag = f\"+|{file.split('.')[0]}|\"\n",
    "        delete_tag = f\"-|{file.split('.')[0]}|\"\n",
    "        \n",
    "        # insert-only operations\n",
    "        init_ops += [insert_tag+'|'.join(i) for i in data]\n",
    "        \n",
    "        # random-update operations\n",
    "        rand_insert_num = int(max(10, int(len(data) * rand_insert_ratio)))\n",
    "        rand_insert_ops = random.sample(data, rand_insert_num)\n",
    "        rand_delete_ops = random.sample(rand_insert_ops, int(rand_insert_num*0.5))\n",
    "        rand_delete_ops += random.sample(data, int(rand_insert_num*0.5))\n",
    "\n",
    "        tmp_update_ops = []\n",
    "        for u in rand_insert_ops:\n",
    "            tmp_update_ops.append(insert_tag + '|'.join(u))\n",
    "        for u in rand_delete_ops:\n",
    "            tmp_update_ops.append(delete_tag + '|'.join(u))\n",
    "        update_ops += tmp_update_ops\n",
    "        \n",
    "    with open(folder + f\"/ops_sf{sf}_init.txt\" , 'w') as op_file:\n",
    "        for op in init_ops:\n",
    "            op_file.write(op + '\\n')\n",
    "\n",
    "    random.shuffle(update_ops)\n",
    "    with open(folder + f\"/ops_sf{sf}_update.txt\" , 'w') as op_file:\n",
    "        for op in update_ops:\n",
    "            op_file.write(op + '\\n')\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Split Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Convert the operation to SQL insert/delete statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames = {\n",
    "    \"nation\": [\"N_NATIONKEY\", \"N_NAME\", \"N_REGIONKEY\", \"N_COMMENT\"],\n",
    "    \"customer\": [\"C_CUSTKEY\", \"C_NAME\", \"C_ADDRESS\", \"C_NATIONKEY\", \"C_PHONE\", \"C_ACCTBAL\", \"C_MKTSEGMENT\", \"C_COMMENT\"],\n",
    "    \"orders\": [\"O_ORDERKEY\", \"O_CUSTKEY\", \"O_ORDERSTATUS\", \"O_TOTALPRICE\", \"O_ORDERDATE\", \"O_ORDERPRIORITY\", \"O_CLERK\", \"O_SHIPPRIORITY\", \"O_COMMENT\"],\n",
    "    \"lineitem\": [\"L_ORDERKEY\", \"L_PARTKEY\", \"L_SUPPKEY\", \"L_LINENUMBER\", \"L_QUANTITY\", \"L_EXTENDEDPRICE\", \"L_DISCOUNT\", \"L_TAX\", \"L_RETURNFLAG\", \"L_LINESTATUS\", \"L_SHIPDATE\", \"L_COMMITDATE\", \"L_RECEIPTDATE\", \"L_SHIPINSTRUCT\", \"L_SHIPMODE\", \"L_COMMENT\"]\n",
    "}\n",
    "\n",
    "def convert_to_sql(ops):\n",
    "    \n",
    "    sql_statements = []\n",
    "    \n",
    "    for op in ops:\n",
    "        table_name = op.split('|')[1]\n",
    "        values = op.split('|')[2:]\n",
    "        if '\\n' in values:\n",
    "            values.remove('\\n')\n",
    "        if op.startswith(\"+\"):\n",
    "            values = [f\"'{v}'\" if not v.isdigit() else v for v in values]\n",
    "            values = [f\"'{v}'\" if v == \"null\" else v for v in values]\n",
    "            sql = f\"INSERT INTO {table_name} VALUES ({', '.join(values)});\"\n",
    "        elif op.startswith(\"-\"):\n",
    "            conditions = [f\"{fieldnames[table_name][i]} = '{c}'\" if not c.isdigit() else f\"{fieldnames[table_name][i]} = {c}\" for i, c in enumerate(values)]\n",
    "            sql = f\"DELETE FROM {table_name} WHERE { ' AND '.join(conditions)};\"\n",
    "        sql_statements.append(sql)\n",
    "    \n",
    "    return sql_statements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sf in ['0.2', '0.4', '0.6', '0.8', '1.0']:\n",
    "    folder = \"tpch_datasets/data_sf\" + sf\n",
    "    \n",
    "    for fname in [f'ops_sf{sf}_init.txt', f'ops_sf{sf}_update.txt']:\n",
    "        sql_statements = convert_to_sql(folder + \"/\" + fname)\n",
    "\n",
    "        with open(\"mysql_test/\" + fname.replace(\"txt\", \"sql\") , 'w') as sql_file:\n",
    "            sql_file.write(\"USE tpch;\\n\")\n",
    "            for statement in sql_statements:\n",
    "                sql_file.write(statement + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of monitoring points\n",
    "n_split = 5 \n",
    "\n",
    "sf = 0.1\n",
    "\n",
    "folder = \"tpch_datasets/data_sf\" + str(sf)\n",
    "fname = f\"/ops_sf{sf}_update.txt\"\n",
    "\n",
    "def split_data(data, n_subset=5):\n",
    "    data_splits = np.array_split(data, n_subset)\n",
    "    return [list(split) for split in data_splits]\n",
    "\n",
    "update_ops = read_txt_file(folder + fname)\n",
    "subsets = split_data(update_ops, 5)\n",
    "\n",
    "for i in range(5):\n",
    "    with open(folder + fname.replace(\".txt\", f\"_{(i+1)/n_split}.txt\") , 'w') as op_file:\n",
    "        for s in subsets[:i+1]:\n",
    "            for op in s:\n",
    "                op_file.write(op)\n",
    "\n",
    "    with open(\"mysql_test/\" + fname.replace(\".txt\", f\"_{(i+1)/n_split}.sql\") , 'w') as sql_file:\n",
    "        ops = subsets[i]\n",
    "        sql_statements = convert_to_sql(ops)\n",
    "        sql_file.write(\"USE tpch;\\n\")\n",
    "        for statement in sql_statements:\n",
    "            sql_file.write(statement + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
